{
    "version": "https://jsonfeed.org/version/1",
    "title": "Nineko's Blog • All posts by \"note\" category",
    "description": "",
    "home_page_url": "https://nineko.github.io",
    "items": [
        {
            "id": "https://nineko.github.io/2020/08/05/Residual-Dense/",
            "url": "https://nineko.github.io/2020/08/05/Residual-Dense/",
            "title": "[筆記]序列模型(3)-Residual & Dense",
            "date_published": "2020-08-05T07:10:33.000Z",
            "content_html": "<h1>目錄</h1>\n<p><a href=\"https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/\">基本序列模型</a></p>\n<p><a href=\"https://nineko.github.io/2020/08/04/Note-Multi-Scale/\">多尺度架構</a></p>\n<p><strong>Residual &amp; Dense</strong></p>\n<h1>Residual</h1>\n<hr>\n<p>  Residual 為 <a href=\"https://arxiv.org/abs/1512.03385\">ResNet</a> 中提出的一種架構，中文譯為殘差，其提出的主因是為了解決網路堆疊太深而產生的退化問題，沒錯，網路不是愈深愈好，退化並非 overfitting ，而是誤差確實的提高，這在 ResNet 的論文中有詳細的說明，其實驗證明若只是單純的堆疊網路並不會都是帶來正向的結果。</p>\n<p><img src=\"M1.png\" alt=\"image\"></p>\n<p>  為了解決這種情形，殘差的概念被提出，殘差的想法為 ── 本來神經網路學習的過程可以看成找尋一個適當的函式來滿足你的輸入及輸出，假設輸入為 <code>x</code> ，想要學習的函數為 <code>H(x)</code> ，那我們今天修改下學習的目標，從 <code>H(x)</code>  改成 <code>H(x)-x</code> ，並假設新的目標函式為 <code>F(x)</code> ，那麼我們可以列出式子 :  <code>F(x)=H(x)-x</code> ，移項後就變成 <code>F(x)+x=H(x)</code> ，也就是我們可以利用訓練 <code>F(x)+x</code>  可以跟原本的 <code>H(x)</code>  視為等價。</p>\n<p><img src=\"M2.png\" alt=\"image\"></p>\n<p>  那麼為甚麼這麼做可以解決網路太深的退化問題呢？若依照論文的解釋，它們認為要找出 <code>F(x)</code>  的最優解會比 <code>H(x)</code>  來得容易，因為 <code>F(x)</code>  是針對誤差的誤差 (感覺好繞) 進行最佳化，所以對於變化更加的敏感，而且就算訓練不好，因為還有本來的 <code>x</code> ，所以不會太影響本來的結果；若上述你已可以理解，那麼非常好，不過這邊還有我依照自己理解的白話解釋，我從特徵角度著手，隨著網路逐層加深，或許有些有用的，但是權重不怎麼高的特徵會被磨滅、被覆蓋等情形發生，表現在結果上便是誤差的提高，也就是退化的真相，殘差架構將前面找出的特徵整組加入目前所計算的特徵上，這意味著不需要擔心這層訓練走歪太多，再不濟還有前一層的結果作為一個基準，以上是我對殘差架構的一點說明及理解，若還是不太清楚，那麼開始動手實做看看，或許你可以找出你自己的理解方式，接下來將會說明該如何實做一個簡單的殘差區塊。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Conv2D,Input,Add,Activation</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ResidualBlock</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">15</span>,<span class=\"number\">15</span>,<span class=\"number\">3</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#  1x1 卷積層</span></span><br><span class=\"line\">    conv1 = Conv2D(<span class=\"number\">10</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_1&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\">#  3x3 卷積層</span></span><br><span class=\"line\">    conv2 = Conv2D(<span class=\"number\">20</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_2&#x27;</span>)(conv1)  </span><br><span class=\"line\">    <span class=\"comment\">#  5x5 卷積層</span></span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">10</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_3&#x27;</span>)(conv2)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 - 進行 concatenate</span></span><br><span class=\"line\">    model_output= Add([conv1, conv3] ,name=<span class=\"string\">&#x27;output&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    model_output = Activation(<span class=\"string\">&#x27;relu&#x27;</span>)(model_output)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"M3.png\" alt=\"image\"></p>\n<p>  建構起來非常的簡單，範例中 <code>model_input</code>  經過一層 <code>conv1</code>  後進入 Residual block，因為我們需要將 <code>conv1</code>  的結果向後傳遞與 Residual block 的輸出相加，所以在 block 的輸出層時 (範例中的 <code>conv3</code> )，要注意將維度調整為與 <code>conv1</code>  相同，否則無法正確相加。</p>\n",
            "tags": [
                "DNN"
            ]
        },
        {
            "id": "https://nineko.github.io/2020/08/04/Note-Multi-Scale/",
            "url": "https://nineko.github.io/2020/08/04/Note-Multi-Scale/",
            "title": "[筆記]序列模型(2)-多尺度架構",
            "date_published": "2020-08-04T03:15:06.000Z",
            "content_html": "<h1>目錄</h1>\n<p><a href=\"https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/\">基本序列模型</a></p>\n<p><strong>多尺度架構</strong></p>\n<p><a href=\"https://nineko.github.io/2020/08/05/Residual-Dense/\">Residual &amp; Dense</a></p>\n<h1>多尺度架構</h1>\n<hr>\n<p>  多尺度架構能夠在 GoogLeNet 中的 Inception 架構中廣泛看到，其概念想法為利用不同卷積核大小的卷積層來給予網路更多的選擇去擷取適當的特徵，在架構上，我們能夠把多尺度架構分成兩個步驟──分散及匯集；分散指的是將輸入分散至不同大小卷積核的卷積層，匯集則是將不同卷積層的結果重新組裝成一個張量，所以在進行卷積計算時，會利用 Padding 來確保輸出張量的長寬是一致的，圖解的話像是這種感覺。</p>\n<p><img src=\"M1.png\" alt=\"image\"></p>\n<p>  可以看到為了進行多尺度的計算，我們無法去操作特徵圖的長寬，這意味著龐大的參數計算，為了緩和計算負擔，我們可以利用多次卷積降低兩維的大小，將資訊累積在深度上，多尺度計算時也可以利用 1x1 的卷積核來降低深度維度，使用這些技巧來建構多尺度架構，在結果與參數使用量上取得適當的平衡吧。</p>\n<p>接下來將會用簡單的範例來示範如何建構一個多尺度架構。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Conv2D,Input,concatenate</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">MultiScaleModel</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">15</span>,<span class=\"number\">15</span>,<span class=\"number\">3</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#  1x1 卷積層</span></span><br><span class=\"line\">    conv1 = Conv2D(<span class=\"number\">10</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_1_1&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\">#  3x3 卷積層</span></span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">20</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_3_3&#x27;</span>)(model_input)  </span><br><span class=\"line\">    <span class=\"comment\">#  5x5 卷積層</span></span><br><span class=\"line\">    conv5 = Conv2D(<span class=\"number\">30</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_5_5&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 - 進行 concatenate</span></span><br><span class=\"line\">    model_output= concatenate([conv1, conv3, conv5] , axis=<span class=\"number\">3</span>,name=<span class=\"string\">&#x27;output&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<p>使用 <code>model.summary()</code>  可以更清楚的看到架構。</p>\n<p><img src=\"M2.png\" alt=\"image\"></p>\n<p>這裡值得一提的是，在進行 concatenate 時，需要指定接合的維度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.layers.Concatenate(axis=<span class=\"number\">-1</span>, **kwargs)</span><br></pre></td></tr></table></figure>\n<p>在這裡因為我們的卷積層輸出為 (None,15,15,10) 、 (None,15,15,20) 、 (None,15,15,30) ，需要接合的為第三維度，所以需要設定 <code>axis=3</code> ，若想了解更多資訊，可以參考 <a href=\"https://keras.io/api/layers/merging_layers/concatenate/\">Keras 手冊</a></p>\n<p>  在範例中，我們只建構了一層最簡單的多尺度架構，在使用上這樣的一層只是一個 Block ，利用堆疊這些 Block 如同在建構一般卷積層一般，更深的層數意味著更複雜的特徵組合，然而比起普通卷積，多尺度的一層中包含了不同卷積核的特徵，多層疊加下會得到更複雜的特徵，給予我們設計網路架構時多了一種思路。</p>\n",
            "tags": [
                "DNN"
            ]
        },
        {
            "id": "https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/",
            "url": "https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/",
            "title": "[筆記]序列模型(1)-基本序列模型",
            "date_published": "2020-08-03T07:55:22.000Z",
            "content_html": "<h1>目錄</h1>\n<p><strong>基本序列模型</strong></p>\n<p><a href=\"https://nineko.github.io/2020/08/04/Note-Multi-Scale/\">多尺度架構</a></p>\n<p><a href=\"https://nineko.github.io/2020/08/05/Residual-Dense/\">Residual &amp; Dense</a></p>\n<h1>基本序列模型</h1>\n<hr>\n<p>  序列模型為建構深度學習網路時最直觀也最簡便的方式，在大多數時候，序列模型往往能夠讓你在解決問題上提供一個初步的解決方案。</p>\n<p>不過儘管它如此單純，依舊值得好好研究，在本篇中將會從最基本的開始說明，也就是沒有任何特別操作，單純一層疊一層的方式來建構深度學習網路。</p>\n<p>這種方式在初期被大量使用，它方便架設，也很容易理解，但是它因簡單的架構，無法處理太過於複雜的特徵，也沒有任何機制去降低計算量，可說是有利有弊。</p>\n<h2>建構</h2>\n<hr>\n<h3>全連接層</h3>\n<hr>\n<p>在這個筆記裡，範例皆為 Keras 實做版本，Keras 版本為 2.4.3 ，基底 Tensorflow 版本為 2.3.0 。</p>\n<p>假設我們想要建構一個輸入長度為 100 的向量，經過兩層輸出為 50 的隱藏層後，最後輸出長度為 10 的向量。</p>\n<p><img src=\"M1.png\" alt=\"image\"></p>\n<p>在 Keras 中，我們可以使用很簡單的方式來建構一個序列模型。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Input,Dense</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">SequenceModel</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">100</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 隱藏層 1 </span></span><br><span class=\"line\">    hidden = Dense(<span class=\"number\">50</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>,name=<span class=\"string\">&quot;hidden_1&quot;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\"># 隱藏層 2 </span></span><br><span class=\"line\">    hidden = Dense(<span class=\"number\">50</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>,name=<span class=\"string\">&quot;hidden_2&quot;</span>)(hidden)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 </span></span><br><span class=\"line\">    model_output = Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>,name=<span class=\"string\">&quot;output&quot;</span>)(hidden)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<p>  可以看到由 Keras 建構只需要專心建構網路的架構，而不需要做 Weight 及 Bias 的數量及初始化定義，它會以預設的參數進行建構，若要修改也可以帶入引數進行設定。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">keras.layers.Dense(units, </span><br><span class=\"line\">                   activation=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   use_bias=<span class=\"literal\">True</span>, </span><br><span class=\"line\">                   kernel_initializer=<span class=\"string\">&#x27;glorot_uniform&#x27;</span>, bias_initializer=<span class=\"string\">&#x27;zeros&#x27;</span>, </span><br><span class=\"line\">                   kernel_regularizer=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   bias_regularizer=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   activity_regularizer=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   kernel_constraint=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   bias_constraint=<span class=\"literal\">None</span>)</span><br></pre></td></tr></table></figure>\n<p>若要了解更詳細的設定可以查看 <a href=\"https://keras.io/api/layers/core_layers/dense/\">Keras 手冊</a></p>\n<h3>卷積層</h3>\n<hr>\n<p>  若要建構一個 DNN ，卷積層是必須的，與全連接層相同的做法，只是呼叫的函式不同而已。</p>\n<p>  在接下來的範例中，輸入張量為 100x100x3 ，經過兩層 50 個卷積核為 3x3 ，Stride 為 2 ，不使用 Padding 的卷積層後，進行 Flatten ，最後再接入向量長度為 10 的輸出層。</p>\n<p><img src=\"M2.png\" alt=\"image\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Input,Dense,Conv2D</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">SequenceModel</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">100</span>,<span class=\"number\">100</span>,<span class=\"number\">3</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 隱藏層 1 </span></span><br><span class=\"line\">    hidden = Conv2D(<span class=\"number\">50</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;valid&#x27;</span>,name=<span class=\"string\">&#x27;hidden_1&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\"># 隱藏層 2 </span></span><br><span class=\"line\">    hidden = Conv2D(<span class=\"number\">50</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;valid&#x27;</span>,name=<span class=\"string\">&#x27;hidden_2&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\"># Flatten</span></span><br><span class=\"line\">    Flatten_layer = Flatten(name=<span class=\"string\">&#x27;flatten&#x27;</span>)(hidden)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 </span></span><br><span class=\"line\">    model_output = Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>,name=<span class=\"string\">&quot;output&quot;</span>)(Flatten_layer)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<p>同樣，這只是最基本的應用，若要更進階的使用請詳看 <a href=\"https://keras.io/api/layers/convolution_layers/\">Keras 手冊</a></p>\n<p>  以上，我們已經知道該怎麼建構卷積層及全連接層，使用這兩者已經可以建構一個影像辨識的應用，像是一開始的 AlexNet 及 VGG 系列都是使用單純的卷積加上全連接層建構而成的，接下來，你可以建構自己的架構嘗試進行影像辨識，可以使用<a href=\"http://yann.lecun.com/exdb/mnist/\">手寫辨識 MINST</a> 或是 <a href=\"https://www.kaggle.com/c/dogs-vs-cats\">Kaggle 的 Dogs v.s Cats</a> 進行練習。</p>\n",
            "tags": [
                "DNN"
            ]
        }
    ]
}