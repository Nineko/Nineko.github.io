{
    "version": "https://jsonfeed.org/version/1",
    "title": "Nineko's Blog • All posts by \"note\" category",
    "description": "",
    "home_page_url": "https://nineko.github.io",
    "items": [
        {
            "id": "https://nineko.github.io/2020/09/04/Python-crawler/",
            "url": "https://nineko.github.io/2020/09/04/Python-crawler/",
            "title": "[筆記]Python應用-爬蟲",
            "date_published": "2020-09-04T06:06:56.000Z",
            "content_html": "<h1>何謂爬蟲</h1>\n<hr>\n<p>  爬蟲以白話來說的話就是一個從網頁上抓資料下來的動作，舉一個例子來說 : NVIDIA 最近出了新型號的顯示卡，那麼舊型號勢必會跌價，你今天想研究它跌價的趨勢，並試圖預估最低點撿個便宜，所以你每天都上露天去記錄金額的變化，但是每天上去記錄實在是太麻煩了，不僅容易忘記，記錄還得花上一段時間，面對這種情形，使用爬蟲便會是一個很好的解決方案，爬蟲的用意就是為了解決這種重複性高，大量且單調的工作，有了爬蟲工具，資料科學家便能更方便的蒐集資料，能夠專心的投入在分析上。</p>\n<h1>Python上的爬蟲</h1>\n<hr>\n<p>  那麼在 Python 上該如何撰寫一個爬蟲程式呢？這方面 Python 為我們帶來了完善的解決方案，只要有 requests 加上 BeautifulSoup 兩個套件便可以進行爬蟲程式的撰寫。</p>\n<h2>requests</h2>\n<hr>\n<p>  requests 套件給予我們在 Python 上能夠操作網頁的基本功能，如 GET、 POST 、 PUT、 DELETE、 HEAD、 OPTIONS、 Cookie 操作等功能，在本章節中主要使用 GET 來獲取網頁中的內容，是爬蟲的第一步。</p>\n<p>安裝 requests :</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install requests</span><br></pre></td></tr></table></figure>\n<p>基本 GET 方法使用 :</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"></span><br><span class=\"line\">result = requests.get(<span class=\"string\">&#x27;https://nineko.github.io&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(result.text)</span><br></pre></td></tr></table></figure>\n<p><img src=\"crawler01.png\" alt=\"image\"></p>\n<p>  使用 GET 能夠將網頁的程式碼整個拿下來，所有網頁的資訊都在這裡頭，所以接下的的動作就是來解析並過濾出我們所需要的資訊，而這個則需要方才所說的另一個套件，BeautifulSoup。</p>\n<h2>BeautifulSoup</h2>\n<hr>\n<p>  BeautifulSoup 能夠為我們解析 HTML，將其轉換成能夠快速查找資訊的資料結構供後續操作，並提供相應的查找方式，迅速過濾出使用者想要的標籤，可謂是爬蟲的一大利器。</p>\n<p>安裝 requests :</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install beautifulsoup4</span><br></pre></td></tr></table></figure>\n<p>基本方法使用 :</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"></span><br><span class=\"line\">result = requests.get(<span class=\"string\">&#x27;https://nineko.github.io&#x27;</span>)</span><br><span class=\"line\">soup = BeautifulSoup(result, <span class=\"string\">&#x27;html.parser&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(soup.prettify())</span><br></pre></td></tr></table></figure>\n<p><img src=\"crawler02.png\" alt=\"image\"></p>\n<p>  不要看打印出來看起來只是稍微改了點縮排而已，但是內部已經建構了一個特別的資料結構，我們可以用 BeautifulSoup 提供的 select 方法來提取標籤內容。</p>\n<p>查找標籤 :</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"></span><br><span class=\"line\">result = requests.get(<span class=\"string\">&#x27;https://nineko.github.io&#x27;</span>)</span><br><span class=\"line\">soup = BeautifulSoup(result, <span class=\"string\">&#x27;html.parser&#x27;</span>)</span><br><span class=\"line\">title = soup.select(<span class=\"string\">&quot;h3.home-article-title&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> title:</span><br><span class=\"line\">  print(t.text)</span><br></pre></td></tr></table></figure>\n<p><img src=\"crawler03.png\" alt=\"image\"></p>\n<p>  你看，只要短短的幾行程式碼便可以把我 Blog 文章的標題濾出，不過看到這的人可能會有個疑問，這個 <code>h3.home-article-title</code>  哪來的？現在的 Chrome 或是我常用的 OperaGX 等瀏覽器都有可以觀看程式碼的方式，像是 Chrome 能夠點 <code>F12</code>  進入或是在你想看的元件上右鍵選單中的檢查元件都可以跳到其程式碼的部分，觀看程式碼便可以得知其目前標籤，這個 <code>h3.home-article-title</code>  便是這麼來的。</p>\n<p><img src=\"crawler04.png\" alt=\"image\"></p>\n<h1>實戰範例</h1>\n<hr>\n<h2>基本單頁</h2>\n<hr>\n<p>  那麼基本工具都準備好了，現在來實戰看看，我們利用一開始的小例子，去露天鎖定 NVIDIA GTX1060 的顯示卡，把商品名與價錢爬下來，GO！</p>\n<p>  首先得先去露天實際搜尋看看整個頁面，檢查一下商品名的標籤。</p>\n<p><img src=\"crawler05.png\" alt=\"image\"></p>\n<p>那我們就以剛剛的思路試著爬爬看</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"></span><br><span class=\"line\">result = requests.get(<span class=\"string\">&#x27;https://find.ruten.com.tw/s/?cateid=0011000500210001&amp;q=GTX1060&#x27;</span>)</span><br><span class=\"line\">soup = BeautifulSoup(result, <span class=\"string\">&#x27;html.parser&#x27;</span>)</span><br><span class=\"line\">title = soup.select(<span class=\"string\">&quot;h5.prod_name&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> title:</span><br><span class=\"line\">  print(t)</span><br></pre></td></tr></table></figure>\n<p><img src=\"crawler06.png\" alt=\"image\"></p>\n<p>  什麼！被拒絕了，不過別緊張，這是因為很多網站都有著檢查 User-Agent 的機制，若你不設定 User-Agent 就會被檔下來，所以我們可以見招拆招，它需要 User-Agent ，我們就給它一個，在這邊我們可以使用自己瀏覽器的 User-Agent ，像我的就是 <code>Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36</code> ，或是使用 Python 的  <code>fake-useragent</code>  套件產生，再者也可以使用現有的一些 User-Agent，像是 Google 就有 GoogleBot 可以使用，我們接下來就使用 GoogleBot 來進行，所以對程式碼進行一些修改後。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># New</span></span><br><span class=\"line\">headers = &#123;<span class=\"string\">&#x27;User-Agent&#x27;</span>:<span class=\"string\">&#x27;GoogleBot&#x27;</span>&#125;</span><br><span class=\"line\"><span class=\"comment\"># Add headers</span></span><br><span class=\"line\">result = requests.get(<span class=\"string\">&#x27;https://find.ruten.com.tw/s/?cateid=0011000500210001&amp;q=GTX1060&#x27;</span>,headers=headers)</span><br></pre></td></tr></table></figure>\n<p><img src=\"crawler07.png\" alt=\"image\"></p>\n<p>  很好！順利爬到了，不過我們可以看到每個商品的  <code>&lt;h5&gt;</code>  中都還包了兩個  <code>&lt;span&gt;</code>  以及一個  <code>&lt;a&gt;</code>  標籤，我們需要的商品名是在  <code>&lt;a&gt;</code>  標籤中，所以我們將 select 的條件再往下一層。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">title = soup.select(<span class=\"string\">&quot;h5.prod_name a&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"crawler08.png\" alt=\"image\"></p>\n<p>  這樣一來我們就可以只獲取  <code>&lt;a&gt;</code>  標籤中的內容，成功的爬到商品名稱，同理，價格的部分也可以以這樣的方式爬到，完整的程式碼如下，其中  <code>price_list = price_list[1::2]</code>  的理由是因為爬下來的資料每筆會有兩個價格，因為有些商品價格會是一個範圍，所以這邊使用  <code>[1::2]</code>  來取偶數項，也就是範圍中較大的價格值 :</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"></span><br><span class=\"line\">headers = &#123;<span class=\"string\">&#x27;User-Agent&#x27;</span>:<span class=\"string\">&#x27;GoogleBot&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">result = requests.get(<span class=\"string\">&#x27;https://find.ruten.com.tw/s/?cateid=0011000500210001&amp;q=GTX1060&#x27;</span>,headers=headers)</span><br><span class=\"line\">soup = BeautifulSoup(result.text, <span class=\"string\">&#x27;html.parser&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">title = soup.select(<span class=\"string\">&#x27;h5.prod_name a&#x27;</span>)</span><br><span class=\"line\">price = soup.select(<span class=\"string\">&quot;ul.price_box li.about span.price&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">title_list = []</span><br><span class=\"line\">price_list = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> title:</span><br><span class=\"line\">    title_list.append(t.text)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> price:</span><br><span class=\"line\">    price_list.append(p.text)</span><br><span class=\"line\"></span><br><span class=\"line\">price_list = price_list[<span class=\"number\">1</span>::<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> t,p <span class=\"keyword\">in</span> zip(title_list,price_list) :</span><br><span class=\"line\"> print(t+<span class=\"string\">&quot;\\t&quot;</span>+p)</span><br></pre></td></tr></table></figure>\n<p><img src=\"crawler09.png\" alt=\"image\"></p>\n<h2>進階強化</h2>\n<hr>\n<p>  剛剛我們成功爬下來單頁的資料，接下來要更加進階了，我們試著爬多頁資料，首先先來看看到第 2 頁網址會發生甚麼變化 :</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#第一頁</span></span><br><span class=\"line\">https://find.ruten.com.tw/s/?cateid=0011000500210001&amp;q=GTX1060&amp;sort=prc%2Fac</span><br><span class=\"line\"><span class=\"comment\">#第二頁</span></span><br><span class=\"line\">https://find.ruten.com.tw/s/?cateid=0011000500210001&amp;p=2&amp;q=GTX1060&amp;sort=prc%2Fac</span><br></pre></td></tr></table></figure>\n<p>  可以明顯的看到多了  <code>&amp;p=2</code>  這個參數，試著調整這個數值的確帶我們到了其他頁，所以我們現在知道  <code>&amp;p=2</code>  代表著第二頁，藉由改變數字可以跳轉至其他頁，那麼問題就簡單了，我們可以修改  <code>requests.get</code>  的網址來獲取其他頁的訊息。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"></span><br><span class=\"line\">headers = &#123;<span class=\"string\">&#x27;User-Agent&#x27;</span>:<span class=\"string\">&#x27;GoogleBot&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">url = <span class=\"string\">&quot;https://find.ruten.com.tw/s/?cateid=0011000500210001&amp;p=&quot;</span></span><br><span class=\"line\">page= <span class=\"number\">1</span></span><br><span class=\"line\">url2= <span class=\"string\">&quot;&amp;q=GTX1060&amp;sort=prc%2Fac&quot;</span></span><br><span class=\"line\">title_list = []</span><br><span class=\"line\">price_list = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span>(<span class=\"literal\">True</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    result = requests.get(url+str(page)+url2,headers=headers)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> result.status_code == requests.codes.ok :</span><br><span class=\"line\">     soup = BeautifulSoup(result.text, <span class=\"string\">&#x27;html.parser&#x27;</span>)</span><br><span class=\"line\">     title = soup.select(<span class=\"string\">&#x27;h5.prod_name a&#x27;</span>)</span><br><span class=\"line\">     price = soup.select(<span class=\"string\">&quot;ul.price_box li.about span.price&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> title:</span><br><span class=\"line\">      title_list.append(t.text)</span><br><span class=\"line\">     <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> price:</span><br><span class=\"line\">      price_list.append(p.text)</span><br><span class=\"line\">     page+=<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">     <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">price_list = price_list[<span class=\"number\">1</span>::<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> t,p <span class=\"keyword\">in</span> zip(title_list,price_list) :</span><br><span class=\"line\"> print(t+<span class=\"string\">&quot;\\t&quot;</span>+p)</span><br></pre></td></tr></table></figure>\n<p>  在這邊我們做了幾點修改，首先是  <code>requests.get</code>  傳入 URL 的組合字串，藉由改變  <code>page</code>  參數來獲取其他頁資料，再來為了遍歷所有頁，使用  <code>while</code>  來持續遍歷，並藉由  <code>requests.get</code>  的  <code>status_code</code>  ，成功為  <code>200</code>  做為是否繼續的判斷點，如此一來便可以將所有搜尋資料通通都爬下來了，接下來為了方便分析，我們要將其存成檔案，我們可以利用  <code>csv</code>  套件將其存成 .csv 檔，或是使用  <code>pandas</code>  套件處理，我們這裡採用  <code>pandas</code>  來進行處理， <code>pandas</code>  功能強大，雖然我們今天只要處理兩欄的小資料，但若之後需要處理較多欄位的大資料會比較方便， <code>pandas</code>  的安裝及使用我會到 深度學習的訓練資料準備 的文章內再解說，這邊就直接使用。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;商品名&#x27;</span>:title_list,</span><br><span class=\"line\">                   <span class=\"string\">&#x27;價格&#x27;</span>  :price_list&#125;)</span><br><span class=\"line\">df.to_csv(<span class=\"string\">&#x27;GTX1060.csv&#x27;</span>, encoding=<span class=\"string\">&#x27;utf_8_sig&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"crawler10.png\" alt=\"image\"></p>\n",
            "tags": [
                "Python"
            ]
        },
        {
            "id": "https://nineko.github.io/2020/08/06/Note-Py-TrainingData/",
            "url": "https://nineko.github.io/2020/08/06/Note-Py-TrainingData/",
            "title": "Note_Py_TrainingData",
            "date_published": "2020-08-06T08:53:59.000Z",
            "content_html": "<h1>訓練資料導入</h1>\n<hr>\n<p>紀錄為了能將訓練資料序列化所進行的轉換動作；<br>\n利用 Pandas package 將資料轉換成 Data frame，再以 PKL 檔案格式儲存。</p>\n<h1>資料預處理</h1>\n<hr>\n<p>在本例中以 CAD 模型的圖像資料為例，每個類別中有不同虛擬相機視點的組合，每種組合中都儲存了模型的圖像及模型線框的圖像如下的資料夾結構：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Class 1 --</span><br><span class=\"line\"> model --</span><br><span class=\"line\">  Angle 1--</span><br><span class=\"line\">      img</span><br><span class=\"line\">  Angle 2--</span><br><span class=\"line\">      img</span><br><span class=\"line\"> wireframe --</span><br><span class=\"line\">  Angle 1--</span><br><span class=\"line\">      img</span><br><span class=\"line\">  Angle 2--</span><br><span class=\"line\">      img</span><br><span class=\"line\">Class 2 --</span><br><span class=\"line\">  ... ...</span><br></pre></td></tr></table></figure>\n<p>當然只要有辦法處裡，用任何結構儲存都行</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">c = list(zip(ModelImageList, WireImageList))</span><br><span class=\"line\">shuffle(c)</span><br><span class=\"line\">ModelImageList, WireImageList = zip(*c)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">final_dict = &#123;</span><br><span class=\"line\">  <span class=\"string\">&quot;Model&quot;</span>    : ModelImageList,</span><br><span class=\"line\">  <span class=\"string\">&quot;Wireframe&quot;</span>: WireImageList</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">df = pd.DataFrame(final_dict)</span><br><span class=\"line\">df.to_pickle(saveName)</span><br></pre></td></tr></table></figure>\n<p>此為轉換成 PKL 檔案的主要指令<br>\n另外也能夠存成 CSV 檔案<br>\n不過若是儲存的內容屬於多維度的資訊<br>\n使用 PKL 會比較好一些</p>\n<h2 id=\"訓練時使用pkl檔案\"><a class=\"markdownIt-Anchor\" href=\"#訓練時使用pkl檔案\">#</a> 訓練時使用 PKL 檔案</h2>\n<p>使用 keras 的 Sequence 類，能夠建立一個能夠將資料在訓練中依照 batch 數量即時讀入並進入訓練的類別<br>\n基本架構為</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DataSequence</span>(<span class=\"params\">Sequence</span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, df, batch_size, mode=<span class=\"string\">&#x27;train&#x27;</span></span>):</span></span><br><span class=\"line\">        self.df = df</span><br><span class=\"line\">        self.bsz = batch_size</span><br><span class=\"line\">        self.mode = mode </span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> int(math.ceil(len(self.df) / float(self.bsz)))</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span></span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<p>df 為前述的 PKL 檔案，能利用 data = pd.read_pickle (PKL_Dir) 進行讀檔後輸入<br>\n讀入的 df 中包含許多資料，在 <strong>init</strong> 中進行進一步的讀取為</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, df, batch_size, mode=<span class=\"string\">&#x27;train&#x27;</span></span>):</span></span><br><span class=\"line\">    self.df = df</span><br><span class=\"line\">    self.bsz = batch_size</span><br><span class=\"line\">    self.mode = mode </span><br><span class=\"line\">    self.Wireframe_list = self.df[<span class=\"string\">&#x27;Wireframe&#x27;</span>].tolist()</span><br><span class=\"line\">    self.Model_list = self.df[<span class=\"string\">&#x27;Model&#x27;</span>].tolist()</span><br><span class=\"line\">    self.indexes = np.arange(len(self.df[<span class=\"string\">&#x27;Model&#x27;</span>].tolist()))</span><br></pre></td></tr></table></figure>\n<p>其中因為本例中 PKL 檔案儲存的為影像的路徑，為 list 資料，所以使用 .tolist () 進行讀取，若儲存的為數值型資料，能夠利用 .values 進行讀取<br>\n此外再新增 on_epoch_end () 使整個 epoch 的資料使用完時，能夠在自動洗亂一次</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">on_epoch_end</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">    self.indexes = np.arange(len(self.Model_list))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> self.mode == <span class=\"string\">&#x27;train&#x27;</span>:</span><br><span class=\"line\">    np.random.shuffle(self.indexes)</span><br></pre></td></tr></table></figure>\n<p>並新增獲取 batch 內資料的 get_batch_Models () 及 get_batch_Wireframes ()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_batch_Models</span>(<span class=\"params\">self, idx</span>):</span></span><br><span class=\"line\">    Batch_indexes = self.indexes[idx*self.bsz:(idx+<span class=\"number\">1</span>)*self.bsz]</span><br><span class=\"line\">    Train_list_temp = [self.Model_list[k] <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> Batch_indexes]</span><br><span class=\"line\">    batch_model = np.array([load_image(im,color_mode=<span class=\"string\">&#x27;rgb&#x27;</span>) <span class=\"keyword\">for</span> im <span class=\"keyword\">in</span> Train_list_temp])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> batch_model</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_batch_Wireframes</span>(<span class=\"params\">self, idx</span>):</span></span><br><span class=\"line\">    Batch_indexes = self.indexes[idx*self.bsz:(idx+<span class=\"number\">1</span>)*self.bsz]</span><br><span class=\"line\">    Answer_list_temp = [self.Wireframe_list[k] <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> Batch_indexes]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.array([load_image(im,color_mode=<span class=\"string\">&#x27;grayscale&#x27;</span>) <span class=\"keyword\">for</span> im <span class=\"keyword\">in</span> Answer_list_temp])</span><br></pre></td></tr></table></figure>\n<p>最後完善 <strong>getitem</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span></span><br><span class=\"line\">    batch_model = self.get_batch_Models(idx)</span><br><span class=\"line\">    batch_wireframe = self.get_batch_Wireframes(idx)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> batch_model,batch_wireframe</span><br></pre></td></tr></table></figure>",
            "tags": [
                "DNN"
            ]
        },
        {
            "id": "https://nineko.github.io/2020/08/05/Residual-Dense/",
            "url": "https://nineko.github.io/2020/08/05/Residual-Dense/",
            "title": "[筆記]序列模型(3)-Residual & Dense",
            "date_published": "2020-08-05T07:10:33.000Z",
            "content_html": "<h1>目錄</h1>\n<p><a href=\"https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/\">基本序列模型</a></p>\n<p><a href=\"https://nineko.github.io/2020/08/04/Note-Multi-Scale/\">多尺度架構</a></p>\n<p><strong>Residual &amp; Dense</strong></p>\n<h1>Residual</h1>\n<hr>\n<p>  Residual 為 <a href=\"https://arxiv.org/abs/1512.03385\">ResNet</a> 中提出的一種架構，中文譯為殘差，其提出的主因是為了解決網路堆疊太深而產生的退化問題，沒錯，網路不是愈深愈好，退化並非 overfitting ，而是誤差確實的提高，這在 ResNet 的論文中有詳細的說明，其實驗證明若只是單純的堆疊網路並不會都是帶來正向的結果。</p>\n<p><img src=\"M1.png\" alt=\"image\"></p>\n<p>  為了解決這種情形，殘差的概念被提出，殘差的想法為 ── 本來神經網路學習的過程可以看成找尋一個適當的函式來滿足你的輸入及輸出，假設輸入為 <code>x</code> ，想要學習的函數為 <code>H(x)</code> ，那我們今天修改下學習的目標，從 <code>H(x)</code>  改成 <code>H(x)-x</code> ，並假設新的目標函式為 <code>F(x)</code> ，那麼我們可以列出式子 :  <code>F(x)=H(x)-x</code> ，移項後就變成 <code>F(x)+x=H(x)</code> ，也就是我們可以利用訓練 <code>F(x)+x</code>  可以跟原本的 <code>H(x)</code>  視為等價。</p>\n<p><img src=\"M2.png\" alt=\"image\"></p>\n<p>  那麼為甚麼這麼做可以解決網路太深的退化問題呢？若依照論文的解釋，它們認為要找出 <code>F(x)</code>  的最優解會比 <code>H(x)</code>  來得容易，因為 <code>F(x)</code>  是針對誤差的誤差 (感覺好繞) 進行最佳化，所以對於變化更加的敏感，而且就算訓練不好，因為還有本來的 <code>x</code> ，所以不會太影響本來的結果；</p>\n<p>若上述你已可以理解，那麼非常好，不過這邊還有我依照自己理解的白話解釋，我從特徵角度著手，隨著網路逐層加深，或許有些有用的，但是權重不怎麼高的特徵會被磨滅、被覆蓋等情形發生，表現在結果上便是誤差的提高，也就是退化的真相，殘差架構將前面找出的特徵整組加入目前所計算的特徵上，這意味著不需要擔心這層訓練走歪太多，再不濟還有前一層的結果作為一個基準，以上是我對殘差架構的一點說明及理解，若還是不太清楚，那麼開始動手實做看看，或許你可以找出你自己的理解方式，接下來將會說明該如何實做一個簡單的殘差區塊。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Conv2D,Input,Add,Activation</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ResidualBlock</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">15</span>,<span class=\"number\">15</span>,<span class=\"number\">3</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#  1x1 卷積層</span></span><br><span class=\"line\">    conv1 = Conv2D(<span class=\"number\">10</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_1&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\">#  3x3 卷積層</span></span><br><span class=\"line\">    conv2 = Conv2D(<span class=\"number\">20</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_2&#x27;</span>)(conv1)  </span><br><span class=\"line\">    <span class=\"comment\">#  5x5 卷積層</span></span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">10</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_3&#x27;</span>)(conv2)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 - 進行 concatenate</span></span><br><span class=\"line\">    model_output= Add([conv1, conv3] ,name=<span class=\"string\">&#x27;output&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    model_output = Activation(<span class=\"string\">&#x27;relu&#x27;</span>)(model_output)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"M3.png\" alt=\"image\"></p>\n<p>  建構起來非常的簡單，範例中 <code>model_input</code>  經過一層 <code>conv1</code>  後進入 Residual block，因為我們需要將 <code>conv1</code>  的結果向後傳遞與 Residual block 的輸出相加，所以在 block 的輸出層時 (範例中的 <code>conv3</code> )，要注意將維度調整為與 <code>conv1</code>  相同，否則無法正確相加。</p>\n<p>相加的方法為使用 <code>keras.layers</code>  中的 <code>Add</code>  類別</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.layers.Add(**kwargs)</span><br></pre></td></tr></table></figure>\n<p>詳細的說明可以參照 <a href=\"https://keras.io/api/layers/merging_layers/concatenate/\">Keras 手冊</a></p>\n<h1>Dense</h1>\n<hr>\n<p>  Dense 方法則是可以在 <a href=\"https://arxiv.org/abs/1608.06993\">DenseNet</a> 中看到，其想法與殘差類似，但是不像殘差使用相加的操作，Dense 方法中採用了 concatenate 的方式，並且操作的不只單單前一層，而是前面的所有層，這大大的增強了所有可能的特徵，並且因其採用 concatenate 的方式，所以特徵能夠完全的保留並參予計算。</p>\n<p><img src=\"M4.png\" alt=\"image\"></p>\n<p>  雖然看起來很複雜，但是其實非常好理解，與殘差的想法相同，為了不讓網路在計算時失去有用的特徵，所以將先前找到的特徵加入以防消失，只不過 Dense 方法做得更絕而已，它乾脆一股腦兒的把之前計算的通通併在一塊兒，連同這層的輸出一併餵入下一層。</p>\n<p>  已經有了殘差的概念，理解 Dense 應該不會太困難，接下來就直接進行實做，在範例中，只會實做滿足 Dense 概念的小區塊，若需要看它實際的應用成果，建議查看 <a href=\"https://arxiv.org/abs/1608.06993\">DenseNet</a> ，論文中除了 Dense 架構外，還有著很多特別有趣的操作。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Conv2D,Input,concatenate,Activation</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">DenseBlock</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">15</span>,<span class=\"number\">15</span>,<span class=\"number\">3</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#  卷積層 1</span></span><br><span class=\"line\">    conv1 = Conv2D(<span class=\"number\">10</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_1&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\">#  將前一層的輸出與這層的結果合併</span></span><br><span class=\"line\">    concat1 = concatenate([model_input, conv1] , axis=<span class=\"number\">3</span>,name=<span class=\"string\">&#x27;Concat_1&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#  卷積層 2</span></span><br><span class=\"line\">    conv2 = Conv2D(<span class=\"number\">20</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_2&#x27;</span>)(concat1)  </span><br><span class=\"line\">    <span class=\"comment\">#  將前面兩層的輸出與這層的結果合併</span></span><br><span class=\"line\">    concat2 = concatenate([concat1, conv2] , axis=<span class=\"number\">3</span>,name=<span class=\"string\">&#x27;Concat_2&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#  卷積層 3</span></span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">10</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_3&#x27;</span>)(concat2)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 </span></span><br><span class=\"line\">    model_output= concatenate([concat2, conv3] , axis=<span class=\"number\">3</span>,name=<span class=\"string\">&#x27;Concat_3&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"M5.png\" alt=\"image\"></p>\n<p>  可以看到範例中每個餵入下層卷積的輸入都是由前面層所合併的，但是別看深度愈來愈多，好像會很花計算資源，但其實以生成特徵的花費來看，同樣一個 256 維的特徵，一個是直接計算出 256 維，一個是由前面 4 層的 64 維組裝起來，差異就出來了，使用這種方法，反而可以減少產生特徵的花費，並可以保留前面的特徵，防止其退化，這種操作還是很巧妙的。</p>\n<p>  至此，我們已經說明了序列模型的建構以及兩種架構技巧，對於很多應用來說，這些已經足夠應付，不過該如何變化，還是需要自行去好好研究，甚至模型架構也只是深度學習的一環而已，還有很重要的訓練資料及損失函數等等部分都是進行一個深度學習專案需要考慮的事情，深度學習的坑是很深的，並不是單純的把資料丟進去讓它慢慢跑就好，所以，學無止盡，既然踏入了深度學習這個巨坑，只能持續的接受更多的知識了。</p>\n",
            "tags": [
                "DNN"
            ]
        },
        {
            "id": "https://nineko.github.io/2020/08/04/Note-Multi-Scale/",
            "url": "https://nineko.github.io/2020/08/04/Note-Multi-Scale/",
            "title": "[筆記]序列模型(2)-多尺度架構",
            "date_published": "2020-08-04T03:15:06.000Z",
            "content_html": "<h1>目錄</h1>\n<p><a href=\"https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/\">基本序列模型</a></p>\n<p><strong>多尺度架構</strong></p>\n<p><a href=\"https://nineko.github.io/2020/08/05/Residual-Dense/\">Residual &amp; Dense</a></p>\n<h1>多尺度架構</h1>\n<hr>\n<p>  多尺度架構能夠在 GoogLeNet 中的 Inception 架構中廣泛看到，其概念想法為利用不同卷積核大小的卷積層來給予網路更多的選擇去擷取適當的特徵，在架構上，我們能夠把多尺度架構分成兩個步驟──分散及匯集；分散指的是將輸入分散至不同大小卷積核的卷積層，匯集則是將不同卷積層的結果重新組裝成一個張量，所以在進行卷積計算時，會利用 Padding 來確保輸出張量的長寬是一致的，圖解的話像是這種感覺。</p>\n<p><img src=\"M1.png\" alt=\"image\"></p>\n<p>  可以看到為了進行多尺度的計算，我們無法去操作特徵圖的長寬，這意味著龐大的參數計算，為了緩和計算負擔，我們可以利用多次卷積降低兩維的大小，將資訊累積在深度上，多尺度計算時也可以利用 1x1 的卷積核來降低深度維度，使用這些技巧來建構多尺度架構，在結果與參數使用量上取得適當的平衡吧。</p>\n<p>接下來將會用簡單的範例來示範如何建構一個多尺度架構。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Conv2D,Input,concatenate</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">MultiScaleModel</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">15</span>,<span class=\"number\">15</span>,<span class=\"number\">3</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#  1x1 卷積層</span></span><br><span class=\"line\">    conv1 = Conv2D(<span class=\"number\">10</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_1_1&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\">#  3x3 卷積層</span></span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">20</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_3_3&#x27;</span>)(model_input)  </span><br><span class=\"line\">    <span class=\"comment\">#  5x5 卷積層</span></span><br><span class=\"line\">    conv5 = Conv2D(<span class=\"number\">30</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span> ,name=<span class=\"string\">&#x27;Conv_5_5&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 - 進行 concatenate</span></span><br><span class=\"line\">    model_output= concatenate([conv1, conv3, conv5] , axis=<span class=\"number\">3</span>,name=<span class=\"string\">&#x27;output&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<p>使用 <code>model.summary()</code>  可以更清楚的看到架構。</p>\n<p><img src=\"M2.png\" alt=\"image\"></p>\n<p>這裡值得一提的是，在進行 concatenate 時，需要指定接合的維度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.layers.Concatenate(axis=<span class=\"number\">-1</span>, **kwargs)</span><br></pre></td></tr></table></figure>\n<p>在這裡因為我們的卷積層輸出為 (None,15,15,10) 、 (None,15,15,20) 、 (None,15,15,30) ，需要接合的為第三維度，所以需要設定 <code>axis=3</code> ，若想了解更多資訊，可以參考 <a href=\"https://keras.io/api/layers/merging_layers/concatenate/\">Keras 手冊</a></p>\n<p>  在範例中，我們只建構了一層最簡單的多尺度架構，在使用上這樣的一層只是一個 Block ，利用堆疊這些 Block 如同在建構一般卷積層一般，更深的層數意味著更複雜的特徵組合，然而比起普通卷積，多尺度的一層中包含了不同卷積核的特徵，多層疊加下會得到更複雜的特徵，給予我們設計網路架構時多了一種思路。</p>\n",
            "tags": [
                "DNN"
            ]
        },
        {
            "id": "https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/",
            "url": "https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/",
            "title": "[筆記]序列模型(1)-基本序列模型",
            "date_published": "2020-08-03T07:55:22.000Z",
            "content_html": "<h1>目錄</h1>\n<p><strong>基本序列模型</strong></p>\n<p><a href=\"https://nineko.github.io/2020/08/04/Note-Multi-Scale/\">多尺度架構</a></p>\n<p><a href=\"https://nineko.github.io/2020/08/05/Residual-Dense/\">Residual &amp; Dense</a></p>\n<h1>基本序列模型</h1>\n<hr>\n<p>  序列模型為建構深度學習網路時最直觀也最簡便的方式，在大多數時候，序列模型往往能夠讓你在解決問題上提供一個初步的解決方案。</p>\n<p>不過儘管它如此單純，依舊值得好好研究，在本篇中將會從最基本的開始說明，也就是沒有任何特別操作，單純一層疊一層的方式來建構深度學習網路。</p>\n<p>這種方式在初期被大量使用，它方便架設，也很容易理解，但是它因簡單的架構，無法處理太過於複雜的特徵，也沒有任何機制去降低計算量，可說是有利有弊。</p>\n<h2>建構</h2>\n<hr>\n<h3>全連接層</h3>\n<hr>\n<p>在這個筆記裡，範例皆為 Keras 實做版本，Keras 版本為 2.4.3 ，基底 Tensorflow 版本為 2.3.0 。</p>\n<p>假設我們想要建構一個輸入長度為 100 的向量，經過兩層輸出為 50 的隱藏層後，最後輸出長度為 10 的向量。</p>\n<p><img src=\"M1.png\" alt=\"image\"></p>\n<p>在 Keras 中，我們可以使用很簡單的方式來建構一個序列模型。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Input,Dense</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">SequenceModel</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">100</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 隱藏層 1 </span></span><br><span class=\"line\">    hidden = Dense(<span class=\"number\">50</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>,name=<span class=\"string\">&quot;hidden_1&quot;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\"># 隱藏層 2 </span></span><br><span class=\"line\">    hidden = Dense(<span class=\"number\">50</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>,name=<span class=\"string\">&quot;hidden_2&quot;</span>)(hidden)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 </span></span><br><span class=\"line\">    model_output = Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>,name=<span class=\"string\">&quot;output&quot;</span>)(hidden)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<p>  可以看到由 Keras 建構只需要專心建構網路的架構，而不需要做 Weight 及 Bias 的數量及初始化定義，它會以預設的參數進行建構，若要修改也可以帶入引數進行設定。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">keras.layers.Dense(units, </span><br><span class=\"line\">                   activation=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   use_bias=<span class=\"literal\">True</span>, </span><br><span class=\"line\">                   kernel_initializer=<span class=\"string\">&#x27;glorot_uniform&#x27;</span>, bias_initializer=<span class=\"string\">&#x27;zeros&#x27;</span>, </span><br><span class=\"line\">                   kernel_regularizer=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   bias_regularizer=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   activity_regularizer=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   kernel_constraint=<span class=\"literal\">None</span>, </span><br><span class=\"line\">                   bias_constraint=<span class=\"literal\">None</span>)</span><br></pre></td></tr></table></figure>\n<p>若要了解更詳細的設定可以查看 <a href=\"https://keras.io/api/layers/core_layers/dense/\">Keras 手冊</a></p>\n<h3>卷積層</h3>\n<hr>\n<p>  若要建構一個 DNN ，卷積層是必須的，與全連接層相同的做法，只是呼叫的函式不同而已。</p>\n<p>  在接下來的範例中，輸入張量為 100x100x3 ，經過兩層 50 個卷積核為 3x3 ，Stride 為 2 ，不使用 Padding 的卷積層後，進行 Flatten ，最後再接入向量長度為 10 的輸出層。</p>\n<p><img src=\"M2.png\" alt=\"image\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Input,Dense,Conv2D</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">SequenceModel</span>():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 輸入層</span></span><br><span class=\"line\">    model_input = Input(shape=(<span class=\"number\">100</span>,<span class=\"number\">100</span>,<span class=\"number\">3</span>), name=<span class=\"string\">&#x27;input&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 隱藏層 1 </span></span><br><span class=\"line\">    hidden = Conv2D(<span class=\"number\">50</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;valid&#x27;</span>,name=<span class=\"string\">&#x27;hidden_1&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\"># 隱藏層 2 </span></span><br><span class=\"line\">    hidden = Conv2D(<span class=\"number\">50</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;valid&#x27;</span>,name=<span class=\"string\">&#x27;hidden_2&#x27;</span>)(model_input)</span><br><span class=\"line\">    <span class=\"comment\"># Flatten</span></span><br><span class=\"line\">    Flatten_layer = Flatten(name=<span class=\"string\">&#x27;flatten&#x27;</span>)(hidden)</span><br><span class=\"line\">    <span class=\"comment\"># 輸出層 </span></span><br><span class=\"line\">    model_output = Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>,name=<span class=\"string\">&quot;output&quot;</span>)(Flatten_layer)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<p>同樣，這只是最基本的應用，若要更進階的使用請詳看 <a href=\"https://keras.io/api/layers/convolution_layers/\">Keras 手冊</a></p>\n<p>  以上，我們已經知道該怎麼建構卷積層及全連接層，使用這兩者已經可以建構一個影像辨識的應用，像是一開始的 AlexNet 及 VGG 系列都是使用單純的卷積加上全連接層建構而成的，接下來，你可以建構自己的架構嘗試進行影像辨識，可以使用<a href=\"http://yann.lecun.com/exdb/mnist/\">手寫辨識 MINST</a> 或是 <a href=\"https://www.kaggle.com/c/dogs-vs-cats\">Kaggle 的 Dogs v.s Cats</a> 進行練習。</p>\n",
            "tags": [
                "DNN"
            ]
        }
    ]
}