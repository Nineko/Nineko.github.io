<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="utf-8">
    
    <title>
        [筆記]序列模型(3)-Residual &amp; Dense | Nineko&#39;s Blog
    </title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <link rel="shortcut icon" href="/images/favicon.png">
    
    
<link rel="stylesheet" href="/css/style.css">

    <script id="hexo-configurations">
    let CONFIG = {"hostname":"nineko.github.io","root":"/","localsearch":{"enable":true,"trigger":"auto","unescape":false,"preload":false},"themeInfo":{"name":"ILS","version":"1.1.1","author":"XPoet","repository":"https://github.com/XPoet/hexo-theme-ils"}};
  </script>
<meta name="generator" content="Hexo 5.0.0"></head>


<body>
<div class="page-template">
    <div class="page-top">
        <header class="header-wrapper">

    <div class="header-content">

        <a class="logo-title" href="/">
            Nineko&#39;s Blog
        </a>

        <ul class="menu-list">
            
                <li class="menu-item">
                    <a class=""
                       href="/"
                    >
                        首頁
                    </a>
                </li>
            
                <li class="menu-item">
                    <a class=""
                       href="/archives"
                    >
                        文章總覽
                    </a>
                </li>
            
                <li class="menu-item">
                    <a class=""
                       href="/about"
                    >
                        關於作者
                    </a>
                </li>
            
        </ul>

        <div class="menu-bar">
            <div class="menu-bar-middle"></div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item">
                    <a class=""
                       href="/">首頁</a>
                </li>
            
                <li class="drawer-menu-item">
                    <a class=""
                       href="/archives">文章總覽</a>
                </li>
            
                <li class="drawer-menu-item">
                    <a class=""
                       href="/about">關於作者</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


    </div>

    <div class="page-middle ">

        <main class="main-content normal-code-theme">

            <div class="main-content-left">
                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <h3><a class="title-hover-animation">[筆記]序列模型(3)-Residual &amp; Dense</a></h3>
        </div>

        <div class="meta-info">
            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa fa-calendar-o"></i> 2020-08-05 15:10:33
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fa fa-folder"></i>
            <ul>
                
                    <li>
                        <a href="/categories/Note/">Note</a>
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa fa-tags"></i>
            <ul>
                
                    <li>
                        <a href="/tags/DNN/">DNN</a>
                    </li>
                
            </ul>
        </span>
    
    
</div>
        </div>

        <div class="article-content markdown-body">
            <h1>目錄</h1>
<p><a href="https://nineko.github.io/2020/08/03/Note-BasicSequenceModel/">基本序列模型</a></p>
<p><a href="https://nineko.github.io/2020/08/04/Note-Multi-Scale/">多尺度架構</a></p>
<p><strong>Residual &amp; Dense</strong></p>
<h1>Residual</h1>
<hr>
<p>  Residual 為 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">ResNet</a> 中提出的一種架構，中文譯為殘差，其提出的主因是為了解決網路堆疊太深而產生的退化問題，沒錯，網路不是愈深愈好，退化並非 overfitting ，而是誤差確實的提高，這在 ResNet 的論文中有詳細的說明，其實驗證明若只是單純的堆疊網路並不會都是帶來正向的結果。</p>
<p><img src="M1.png" alt="image"></p>
<p>  為了解決這種情形，殘差的概念被提出，殘差的想法為 ── 本來神經網路學習的過程可以看成找尋一個適當的函式來滿足你的輸入及輸出，假設輸入為 <code>x</code> ，想要學習的函數為 <code>H(x)</code> ，那我們今天修改下學習的目標，從 <code>H(x)</code>  改成 <code>H(x)-x</code> ，並假設新的目標函式為 <code>F(x)</code> ，那麼我們可以列出式子 :  <code>F(x)=H(x)-x</code> ，移項後就變成 <code>F(x)+x=H(x)</code> ，也就是我們可以利用訓練 <code>F(x)+x</code>  可以跟原本的 <code>H(x)</code>  視為等價。</p>
<p><img src="M2.png" alt="image"></p>
<p>  那麼為甚麼這麼做可以解決網路太深的退化問題呢？若依照論文的解釋，它們認為要找出 <code>F(x)</code>  的最優解會比 <code>H(x)</code>  來得容易，因為 <code>F(x)</code>  是針對誤差的誤差 (感覺好繞) 進行最佳化，所以對於變化更加的敏感，而且就算訓練不好，因為還有本來的 <code>x</code> ，所以不會太影響本來的結果；</p>
<p>若上述你已可以理解，那麼非常好，不過這邊還有我依照自己理解的白話解釋，我從特徵角度著手，隨著網路逐層加深，或許有些有用的，但是權重不怎麼高的特徵會被磨滅、被覆蓋等情形發生，表現在結果上便是誤差的提高，也就是退化的真相，殘差架構將前面找出的特徵整組加入目前所計算的特徵上，這意味著不需要擔心這層訓練走歪太多，再不濟還有前一層的結果作為一個基準，以上是我對殘差架構的一點說明及理解，若還是不太清楚，那麼開始動手實做看看，或許你可以找出你自己的理解方式，接下來將會說明該如何實做一個簡單的殘差區塊。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D,Input,Add,Activation</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResidualBlock</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 輸入層</span></span><br><span class="line">    model_input = Input(shape=(<span class="number">15</span>,<span class="number">15</span>,<span class="number">3</span>), name=<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line">    <span class="comment">#  1x1 卷積層</span></span><br><span class="line">    conv1 = Conv2D(<span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span> ,name=<span class="string">&#x27;Conv_1&#x27;</span>)(model_input)</span><br><span class="line">    <span class="comment">#  3x3 卷積層</span></span><br><span class="line">    conv2 = Conv2D(<span class="number">20</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span> ,name=<span class="string">&#x27;Conv_2&#x27;</span>)(conv1)  </span><br><span class="line">    <span class="comment">#  5x5 卷積層</span></span><br><span class="line">    conv3 = Conv2D(<span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span> ,name=<span class="string">&#x27;Conv_3&#x27;</span>)(conv2)</span><br><span class="line">    <span class="comment"># 輸出層 - 進行 concatenate</span></span><br><span class="line">    model_output= Add([conv1, conv3] ,name=<span class="string">&#x27;output&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    model_output = Activation(<span class="string">&#x27;relu&#x27;</span>)(model_output)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="M3.png" alt="image"></p>
<p>  建構起來非常的簡單，範例中 <code>model_input</code>  經過一層 <code>conv1</code>  後進入 Residual block，因為我們需要將 <code>conv1</code>  的結果向後傳遞與 Residual block 的輸出相加，所以在 block 的輸出層時 (範例中的 <code>conv3</code> )，要注意將維度調整為與 <code>conv1</code>  相同，否則無法正確相加。</p>
<p>相加的方法為使用 <code>keras.layers</code>  中的 <code>Add</code>  類別</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.layers.Add(**kwargs)</span><br></pre></td></tr></table></figure>
<p>詳細的說明可以參照 <a target="_blank" rel="noopener" href="https://keras.io/api/layers/merging_layers/concatenate/">Keras 手冊</a></p>
<h1>Dense</h1>
<hr>
<p>  Dense 方法則是可以在 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.06993">DenseNet</a> 中看到，其想法與殘差類似，但是不像殘差使用相加的操作，Dense 方法中採用了 concatenate 的方式，並且操作的不只單單前一層，而是前面的所有層，這大大的增強了所有可能的特徵，並且因其採用 concatenate 的方式，所以特徵能夠完全的保留並參予計算。</p>
<p><img src="M4.png" alt="image"></p>
<p>  雖然看起來很複雜，但是其實非常好理解，與殘差的想法相同，為了不讓網路在計算時失去有用的特徵，所以將先前找到的特徵加入以防消失，只不過 Dense 方法做得更絕而已，它乾脆一股腦兒的把之前計算的通通併在一塊兒，連同這層的輸出一併餵入下一層。</p>
<p>  已經有了殘差的概念，理解 Dense 應該不會太困難，接下來就直接進行實做，在範例中，只會實做滿足 Dense 概念的小區塊，若需要看它實際的應用成果，建議查看 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.06993">DenseNet</a> ，論文中除了 Dense 架構外，還有著很多特別有趣的操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D,Input,concatenate,Activation</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DenseBlock</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 輸入層</span></span><br><span class="line">    model_input = Input(shape=(<span class="number">15</span>,<span class="number">15</span>,<span class="number">3</span>), name=<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line">    <span class="comment">#  卷積層 1</span></span><br><span class="line">    conv1 = Conv2D(<span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span> ,name=<span class="string">&#x27;Conv_1&#x27;</span>)(model_input)</span><br><span class="line">    <span class="comment">#  將前一層的輸出與這層的結果合併</span></span><br><span class="line">    concat1 = concatenate([model_input, conv1] , axis=<span class="number">3</span>,name=<span class="string">&#x27;Concat_1&#x27;</span>)</span><br><span class="line">    <span class="comment">#  卷積層 2</span></span><br><span class="line">    conv2 = Conv2D(<span class="number">20</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span> ,name=<span class="string">&#x27;Conv_2&#x27;</span>)(concat1)  </span><br><span class="line">    <span class="comment">#  將前面兩層的輸出與這層的結果合併</span></span><br><span class="line">    concat2 = concatenate([concat1, conv2] , axis=<span class="number">3</span>,name=<span class="string">&#x27;Concat_2&#x27;</span>)</span><br><span class="line">    <span class="comment">#  卷積層 3</span></span><br><span class="line">    conv3 = Conv2D(<span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span> ,name=<span class="string">&#x27;Conv_3&#x27;</span>)(concat2)</span><br><span class="line">    <span class="comment"># 輸出層 </span></span><br><span class="line">    model_output= concatenate([concat2, conv3] , axis=<span class="number">3</span>,name=<span class="string">&#x27;Concat_3&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=[model_input], outputs=[model_output])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="M5.png" alt="image"></p>
<p>  可以看到範例中每個餵入下層卷積的輸入都是由前面層所合併的，但是別看深度愈來愈多，好像會很花計算資源，但其實以生成特徵的花費來看，同樣一個 256 維的特徵，一個是直接計算出 256 維，一個是由前面 4 層的 64 維組裝起來，差異就出來了，使用這種方法，反而可以減少產生特徵的花費，並可以保留前面的特徵，防止其退化，這種操作還是很巧妙的。</p>
<p>  至此，我們已經說明了序列模型的建構以及兩種架構技巧，對於很多應用來說，這些已經足夠應付，不過該如何變化，還是需要自行去好好研究，甚至模型架構也只是深度學習的一環而已，還有很重要的訓練資料及損失函數等等部分都是進行一個深度學習專案需要考慮的事情，深度學習的坑是很深的，並不是單純的把資料丟進去讓它慢慢跑就好，所以，學無止盡，既然踏入了深度學習這個巨坑，只能持續的接受更多的知識了。</p>

        </div>

        <div class="article-nav">
            
                <div class="article-prev">
                    <a class="prev btn"
                       rel="prev"
                       href="/2020/08/06/Note-Py-TrainingData/"
                    >
                        <i class="fa fa-chevron-left"></i> <span class="post-nav-title-item">Note_Py_TrainingData</span><span class="post-nav-item">上一篇</span>
                    </a>
                </div>
            
            
                <div class="article-next">
                    <a class="next btn"
                       rel="next"
                       href="/2020/08/04/Note-Multi-Scale/"
                    >
                        <span class="post-nav-title-item">[筆記]序列模型(2)-多尺度架構</span><span class="post-nav-item">下一篇</span> <i class="fa fa-chevron-right"></i>
                    </a>
                </div>
            
        </div>

        <div class="comment-container">
            <div class="comments-container">
    
</div>
        </div>
    </div>
</div>


                

            </div>

            
                <div class="main-content-right">
                    
    <div class="sidebar sidebar-post fade-in-down-animation">
        <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">目錄</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">Residual</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">Dense</span></a></li></ol>
    </div>
</div>
    </div>

                </div>
            

        </main>

        <div class="sidebar-tools">
            <div class="tools-container">
    <ul class="tools-list">
        
            <li class="search popup-trigger">
                <i class="fa fa-search"></i>
            </li>
            
<script src="/js/local-search.js"></script>

        
        <li class="mode-toggle">
            <i class="fa fa-moon-o"></i>
        </li>
        
            <li class="rss">
                <a href="/undefined" target="_blank"><i class="fa fa-rss"></i></a>
            </li>
        
    </ul>
</div>

        </div>

        
            <div class="scroll-to-top">
                <ul>
                    <li>
                        <!--<i class="fa fa-caret-up"></i>-->
                        <span class="scroll-percent"></span>
                    </li>
                </ul>
            </div>
        
    </div>

    <div class="page-bottom">
        <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy; 2020 <a href="/">Jing-Rong,Lai</a>
        </div>
        <div class="theme-info info-item">
             <a target="_blank" href="https://hexo.io">Hexo</a>  | 主题 <a
                    href="https://github.com/XPoet/hexo-theme-ils" target="_blank">ILS v1.1.1</a>
        </div>
        
    </div>
</footer>
    </div>
</div>

    <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-icon">
            <i class="fa fa-search"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜尋..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>


<script src="/js/main.js"></script>
<script src="/js/header-shrink.js"></script>
<script src="/js/toggle-mode.js"></script>



    
<script src="/js/scroll-to-top.js"></script>





    
        
<script src="/js/code-copy.js"></script>

    

    
        
<script src="/lib/anime.min.js"></script>
<script src="/js/toc.js"></script>

    




<script src="/js/hexo_resize_image.js"></script>
</body>
</html>